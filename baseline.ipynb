{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import ipdb\n",
    "import skimage\n",
    "from skimage.measure import block_reduce\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "def cqt_specgram(audio, n_bins, bins_per_octave, hop_length, sr, fmin, filter_scale):\n",
    "    '''\n",
    "    :param audio:\n",
    "    :param sr:\n",
    "    :return: shape = (n_bins, t)\n",
    "    '''\n",
    "    c = librosa.cqt(audio, sr = sr, n_bins = n_bins, bins_per_octave = bins_per_octave, hop_length = hop_length,\n",
    "                    fmin = fmin, filter_scale = filter_scale)\n",
    "    mag, phase = librosa.core.magphase(c)\n",
    "    c_p = librosa.amplitude_to_db(mag, amin=1e-13, top_db=120., ref=np.max) / 120.0 + 1.0\n",
    "    return c_p\n",
    "\n",
    "\n",
    "def compute_cqt_spec(audio, n_bins = 70, bins_per_octave=10, hop_length = 512, sr = 16000, fmin = librosa.note_to_hz('C1'),\n",
    "             filter_scale = 0.8):\n",
    "    return cqt_specgram(audio, n_bins, bins_per_octave, hop_length, sr, fmin, filter_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class karplus_strong:\n",
    "    def __init__(self, pitch, sampling_freq, stretch_factor, flag):\n",
    "        \"\"\"Inits the string.\"\"\"\n",
    "        self.pitch = pitch\n",
    "        # self.starting_sample = starting_sample\n",
    "        self.sampling_freq = sampling_freq\n",
    "        self.stretch_factor = stretch_factor\n",
    "        self.flag = flag\n",
    "        self.wavetable = self.init_wavetable()\n",
    "        self.current_sample = 0\n",
    "        self.previous_value = 0\n",
    "\n",
    "\n",
    "    def init_wavetable(self):\n",
    "        \"\"\"Generates a new wavetable for the string.\"\"\"\n",
    "        wavetable_size = int(self.sampling_freq) // int(self.pitch)\n",
    "        if self.flag == 0:\n",
    "            self.wavetable = np.ones(wavetable_size)\n",
    "        else:\n",
    "            self.wavetable = (2 * np.random.randint(0, 2, wavetable_size) - 1).astype(np.float)\n",
    "        return self.wavetable\n",
    "\n",
    "\n",
    "    def get_samples(self):\n",
    "        \"\"\"Returns samples from string.\"\"\"\n",
    "        samples = []\n",
    "        while len(samples) < self.sampling_freq:\n",
    "            if self.flag != 1:\n",
    "                r = np.random.binomial(1, self.flag)\n",
    "                sign = float(r == 1) * 2 - 1\n",
    "                self.wavetable[self.current_sample] = sign * 0.5 * (\n",
    "                self.wavetable[self.current_sample] + self.previous_value)\n",
    "            else:\n",
    "                d = np.random.binomial(1, 1 - 1 / self.stretch_factor)\n",
    "                if d == 0:\n",
    "                    self.wavetable[self.current_sample] = 0.5 * (\n",
    "                    self.wavetable[self.current_sample] + self.previous_value)\n",
    "            samples.append(self.wavetable[self.current_sample])\n",
    "            self.previous_value = samples[-1]\n",
    "            self.current_sample += 1\n",
    "            self.current_sample = self.current_sample % self.wavetable.size\n",
    "        return np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_zeros(image, shape):\n",
    "    result = np.zeros(shape)\n",
    "    result[:image.shape[0],:image.shape[1]] = image\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_params(size):\n",
    "    pitch = np.array([np.random.uniform(20, 2000) for _ in range(size)])\n",
    "    sampling_freq = np.array([np.random.uniform(5, 10) * 1000 for i in range(size)])\n",
    "    stretch_factor = np.array([np.random.uniform(1, 10) for _ in range(size)])\n",
    "    flag = np.array([np.random.uniform(0, 1) for _ in range(size)])\n",
    "    #  ipdb.set_trace()\n",
    "    samples = []\n",
    "    strings = []\n",
    "    cqt_specs = []\n",
    "    for i in range(size):\n",
    "        strings.append(karplus_strong(pitch[i], 2 * sampling_freq[i], stretch_factor[i], 1))\n",
    "        samples.append(strings[i].get_samples())\n",
    "        cqt_spec = compute_cqt_spec(samples[i]).T\n",
    "        padded_cqt = pad_zeros(cqt_spec, (cqt_spec.shape[1], cqt_spec.shape[1]))\n",
    "        cqt_specs.append(padded_cqt)\n",
    "    cqt_specs = np.array(cqt_specs)\n",
    "    print(cqt_specs.shape)\n",
    "    return pitch, sampling_freq, stretch_factor, flag, cqt_specs\n",
    "        \n",
    "\n",
    "def generate_data(file, size):\n",
    "    pitch, sampling_freq, stretch_factor, flag, cqt_specs = sample_params(size)\n",
    "    with open(file, 'wb') as fh:\n",
    "        data_dict = {'parameters' : np.array([pitch, sampling_freq, stretch_factor, flag]).T, 'cqt_spec' : cqt_specs}\n",
    "        pkl.dump(data_dict, fh)\n",
    "    fh.close()\n",
    "    print(file)\n",
    "    \n",
    "    \n",
    "def read_data(file):\n",
    "    with open(file, 'rb') as fh:\n",
    "        data = pkl.loads(fh.read())\n",
    "    fh.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_datasets():\n",
    "    generate_data('eval.pkl', 100)\n",
    "    generate_data('test.pkl', 5000)\n",
    "    generate_data('train.pkl', 50000)\n",
    "\n",
    "    \n",
    "def read_dataset():\n",
    "    return read_data('train.pkl'), read_data('test.pkl'), read_data('eval.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_data('eval_data', 100)\n",
    "create_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, eval_data = read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, parameters, cqt_spectrograms):\n",
    "        super(MyDataset, self).__init__()\n",
    "        \n",
    "        self.parameters = parameters\n",
    "        self.cqt_spec = cqt_spectrograms\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.cqt_spec[i].T, self.parameters[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = MyDataset(parameters=train_data['parameters'], cqt_spectrograms=train_data['cqt_spec'])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = MyDataset(parameters=test_data['parameters'], cqt_spectrograms=test_data['cqt_spec'])\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "evalset = MyDataset(parameters=eval_data['parameters'], cqt_spectrograms=eval_data['cqt_spec'])\n",
    "evalloader = torch.utils.data.DataLoader(evalset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 14 * 14, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 14 * 14)        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs.unsqueeze_(1)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "#        print(\"gradients:\\n\")\n",
    "#        for param in net.parameters():\n",
    "#              print(param.grad)\n",
    "#        print(\"outputs:\\n\")\n",
    "#        print(outputs)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 1:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss))\n",
    "            running_loss = 0.0\n",
    "    print(running_loss)\n",
    "    with open(\"losses.txt\", \"a\") as text_file:\n",
    "        text_file.write(str(\"%.10f\" % running_loss))\n",
    "        text_file.write(\"\\n\")\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-env]",
   "language": "python",
   "name": "conda-env-torch-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
